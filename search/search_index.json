{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PIDSMaker","text":"<p>The first framework designed to build and experiment with provenance-based intrusion detection systems (PIDSs) using deep learning architectures. It provides a single codebase to run most recent state-of-the-art systems and easily customize them to develop new variants.</p>"},{"location":"#purpose","title":"Purpose","text":"<p>\ud83e\udd77 PIDSMaker is an open-source framework designed to be collaboratively developed and maintained by the security research community. It was born out of the observation that recent papers in top-tier security venues often evaluate on the same datasets but differ in labeling strategies and in the implementation of baseline methods.</p> <p>Until now, no standardized open-source framework has existed to facilitate fair comparisons. PIDSMaker addresses this gap by providing the following key features:</p> <ol> <li>Consistent evaluation and benchmarking of SOTA baselines using unified datasets, labeling strategies, and reference implementations.</li> <li>A modular testbed of existing components extracted from published systems, enabling experimentation and the discovery of improved variants.</li> <li>A centralized repository where authors can contribute and share code for new systems, ensuring fair and reproducible benchmarking.</li> </ol>"},{"location":"#supported-pidss","title":"Supported PIDSs","text":"<ul> <li>Velox (USENIX Sec'25): Sometimes Simpler is Better: A Comprehensive Analysis of State-of-the-Art Provenance-Based Intrusion Detection Systems</li> <li>Orthrus (USENIX Sec'25): ORTHRUS: Achieving High Quality of Attribution in Provenance-based Intrusion Detection Systems</li> <li>R-Caid (IEEE S&amp;P'24): R-CAID: Embedding Root Cause Analysis within Provenance-based Intrusion Detection</li> <li>Flash (IEEE S&amp;P'24): Flash: A Comprehensive Approach to Intrusion Detection via Provenance Graph Representation Learning</li> <li>Kairos (IEEE S&amp;P'24): Kairos: Practical Intrusion Detection and Investigation using Whole-system Provenance</li> <li>Magic (USENIX Sec'24): MAGIC: Detecting Advanced Persistent Threats via Masked Graph Representation Learning</li> <li>NodLink (NDSS'24): NODLINK: An Online System for Fine-Grained APT Attack Detection and Investigation</li> <li>ThreaTrace (IEEE TIFS'22): THREATRACE: Detecting and Tracing Host-Based Threats in Node Level Through Provenance Graph Learning</li> </ul>"},{"location":"#citing-the-framework","title":"Citing the Framework","text":"<p>If you use this framework, please cite the following paper: <pre><code>@inproceedings{bilot2025simpler,\n    title={{Sometimes Simpler is Better: A Comprehensive Analysis of State-of-the-Art Provenance-Based Intrusion Detection Systems}},\n    author={Bilot, Tristan and Jiang, Baoxiang and  Li, Zefeng and  El Madhoun, Nour and Al Agha, Khaldoun and Zouaoui, Anis and Pasquier, Thomas},\n    booktitle={Security Symposium (USENIX Sec'25)},\n    year={2025},\n    organization={USENIX}\n}\n</code></pre></p>"},{"location":"contribute/","title":"Deployment","text":""},{"location":"contribute/#sorte-dilapsa-veniam-roganti-undas-tum-ab","title":"Sorte dilapsa veniam roganti undas tum ab","text":"<p>Lorem markdownum. Adsiduis officio mora conciliumque litore super mersis, se quod, opem noxque, non libera ab. Infra opus peregit non. Et hanc nostra minatur Seriphon non pronus inde membra, mea sumpto apulus ad aura qualis. Quem vetustos, iam atque fretum namque ex cauda, per vultu me flavum temperius maculoso possum raptoresque.</p> <p>Perspice et equo fulgura, aut gaudet sunt dumque niveum, fugitque abstrahor saxis fulmineis est esse. Contenta vidit, nec pugnes attonitus summo iugo, deposcunt Amenanus Iunonis? Quaesitamque motae nec: terras heros semper nuper, et inpono gradus, ab.</p>"},{"location":"contribute/#amictus-polenta-carina-admirantibus-faciat","title":"Amictus polenta carina admirantibus faciat","text":"<p>Tum quae convicia studiis: faxo cunas visa clarus, quondam, prius. Novi nata dedit, Poemenis trementi haruspex prodere fontem: e. Nova nubes bracchia aras squamas moraque ad segetes remoto.</p> <pre><code>&gt; Modo dira nostroque congreditur patria si corpus insidior illam lusibus per\n&gt; vim relinquunt talia. Per an supremo gaudet latices *Melaneus* latuerunt tibi\n&gt; petebar, Pirithoi spectacula. Et hectora limina, quaeque, hoc adde precor\n&gt; domosque, sed in dieque, me pinnis desunt, cadunt. Nam fallacis isto talia\n&gt; tenues inferiusque non solito hic matre? Mirum at licet qui cuncta collo\n&gt; [deductus](http://www.fraude-ignipedum.io/) animoque quoque.\n</code></pre> <p>Tamen tenebras sit vacuas ire fecerat deus reddidit sonantia, mite sorores, surrexit removente iussa et, ne. Verba malorum Lycias tempestiva irae ex mentes illo durasse Proteus genitore habes; Phoebes doloris, et rara.</p>"},{"location":"contribute/#sum-ait-othrysque-hoc-unum-convexa-hinc","title":"Sum ait Othrysque hoc unum convexa hinc","text":"<p>Inque inmurmurat at prodere: o viri corpora nuper, ut calido certa cum. Ilios enim iam iniustaque fonti, ab torva. Auro precor solebat vincla septemplice ultra errore crescens nomina dextraque annis praetemptatque carpitque protinus dextra, exanimem, fatentis. Maris rates, et fugant et pia aera rumpere arces laesasque, ira silvas quem ministri variatis Cerberon videntem si.</p> <p>Aureus in tellus deplanxere facti se tuus sive, cecidere quisque, variare pulsatus. Non in gentesque funera sufficiunt detractare illo perlucentibus atlas vel reperta non insula.</p> <p>Noctis tamen. Ora facta armo montis iussae Busirin in flammae inanes.</p> <p>Pia fuerunt movit, terra turba mentis quem cinis cladem, madidum labor ponit clavam. Imo vitamque: diem cum ore intravit pro filia Aiax.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>This project aims to share the latest advancements in PIDS research and must therefore be actively maintained. We need your help for this task!</p> <p>Helpful contributions could be:</p> <ul> <li>Adding a new PIDS</li> <li>Fix a bug</li> <li>Adding new methods (e.g. encoders, decoders, featurization)</li> <li>Update current PIDSs (e.g. some systems may not reflect perfectly their original implementation)</li> <li>Optimizing pieces of code (e.g. you spot a slow operation and found a better alternative)</li> </ul>"},{"location":"contributing/#pro-tips","title":"Pro tips","text":"<p>We provide some tips for those willing to contribute to PIDSMaker.</p> <ul> <li>Use dev container in VSCode: all interactions with PIDSMaker are made via the <code>pids</code> container. For a nicer dev experience, we highly recommend opening your VSCode window directly into the container for debuging purposes. More details here.</li> <li>Avoid concurrent runs from scratch: if you launch 2 runs simultaneously, each with the same <code>build_graphs</code> config and same dataset, the files generated by each run will write concurrently in the same folder, leading to potentially invisible conflicts. To avoid this, you should launch the first run and wait for it to complete the first tasks to not overlap with concurrent runs. Once concurrent tasks are completed, you can run as many parallel runs as long as they start from a task with different arguments. Another way is to launch each run using <code>--restart_from_scratch</code> to ensure no overlap.</li> </ul>"},{"location":"contributing/#contribution-guidelines","title":"Contribution guidelines","text":"<p>Before opening your PR, ensure to perform the following steps.</p>"},{"location":"contributing/#format-all-files","title":"Format all files","text":"<pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"contributing/#run-tests","title":"Run tests","text":"<p>See guidelines.</p>"},{"location":"contributing/#build-documentation","title":"Build documentation","text":"<p>See guidelines.</p>"},{"location":"create-db-from-scratch/","title":"Install a dataset from scratch","text":"<p>PIDSMaker comes by default with pre-processed versions of DARPA datasets. If you want to install them from scratch using the official public files, follow this guide.</p>"},{"location":"create-db-from-scratch/#download-files","title":"Download files","text":"<ol> <li> <p>Create an empty folder <code>DATA_FOLDER</code> and make sure that you have enough space to download the raw data     <pre><code>DATA_FOLDER=./data\nmkdir ${DATA_FOLDER}\n</code></pre></p> </li> <li> <p>Install gdown     <pre><code>pip install gdown\n</code></pre></p> </li> <li> <p>Download dataset</p> <p>For DARPA TC datasets, run:</p> <p><pre><code>./dataset_preprocessing/darpa_tc/scripts/download_DATASET.sh ${DATA_FOLDER}\n</code></pre> where <code>DATASET</code> can be either <code>clearscope_e3</code>, <code>cadets_e3</code>, <code>theia_e3</code>, <code>clearscope_e5</code>, <code>cadets_e5</code> or <code>theia_e5</code> and <code>DATA_FOLDER</code> is the absolute path to the output folder where all raw files will be downloaded.</p> <p>Alternatively, you can download the files manually by selecting download URLs from Google Drive.</p> <p>For DARPA OpTC datasets, run: <pre><code>python ./dataset_preprocessing/optc/download_dataset.py DATASET ${DATA_FOLDER}\n</code></pre> where <code>DATASET</code> can be either <code>optc_h051</code>, <code>optc_h201</code> or <code>optc_h501</code> and <code>DATA_FOLDER</code> is the absolute path to the output folder where all raw files will be downloaded.</p> <p>Note</p> <p>Make sure <code>DATA_FOLDER</code> is empty before downloading and parsing raw data.  Remove all old files before downloading a new dataset.</p> </li> </ol>"},{"location":"create-db-from-scratch/#install-docker-images","title":"Install docker images","text":"<ol> <li> <p>In <code>compose-pidsmaker.yml</code>, uncomment <code>- /path/to/raw/data:/data</code> and set <code>/path/to/raw/data</code> as the DATA_FOLDER where you downloaded the downloaded dataset files (.gz), the java client (tar.gz) and the schema files (.avdl, .avsc)</p> </li> <li> <p>Follow the guidelines to build the docker image and open a shell of pidsmaker container</p> </li> </ol>"},{"location":"create-db-from-scratch/#extract-files-tc","title":"Extract files (TC)","text":"<p>This part is only for DARPA TC datasets (i.e. <code>clearscope_e3</code>, <code>cadets_e3</code>, <code>theia_e3</code>, <code>clearscope_e5</code>, <code>cadets_e5</code> and <code>theia_e5</code>)</p> <p>In the <code>pidscontainer</code>, uncompress the DARPA TC files by running: <pre><code>./dataset_preprocessing/darpa_tc/scripts/uncompress_darpa_files.sh /data/\n</code></pre></p> <p>Note</p> <p>This may take multiple hours depending on the dataset.</p>"},{"location":"create-db-from-scratch/#extract-files-optc","title":"Extract files (OpTC)","text":"<p>This part is only for DARPA OpTC dataset (i.e. <code>optc_h051</code>, <code>optc_h201</code> and <code>optc_h501</code>)</p> <p>In the <code>pidscontainer</code>, extract the files by running: <pre><code>./dataset_preprocessing/optc/extract_data.sh /data/\n</code></pre></p>"},{"location":"create-db-from-scratch/#optional-configurations","title":"Optional configurations","text":"<ul> <li>Set optional configs before filling the database if needed. If using a specific postgres database instead of the postgres docker, update the connection config by setting <code>DATABASE_DEFAULT_CONFIG</code> within <code>pidsmaker/config/pipeline.py</code>.</li> <li>If using a specific postgres database instead of the postgres docker, copy creating_database to your database server and run it to create databases, and then copry creating_tables to your server and run it to create tables.</li> </ul>"},{"location":"create-db-from-scratch/#fill-the-database-tc","title":"Fill the database (TC)","text":"<p>For TC datasets (<code>clearscope_e3</code>, <code>cadets_e3</code>, <code>theia_e3</code>, <code>clearscope_e5</code>, <code>cadets_e5</code> and <code>theia_e5</code>)</p> <p>Still in the container's shell, fill the database for the corresponding dataset by running this command:</p> <p><pre><code>python dataset_preprocessing/darpa_tc/create_database_e5.py orthrus DATASET\n</code></pre> where <code>DATASET</code> can be [<code>CLEARSCOPE_E5</code> | <code>CADETS_E5</code> | <code>THEIA_E5</code>].  Or  <pre><code>python dataset_preprocessing/darpa_tc/create_database_e3.py orthrus DATASET\n</code></pre> where <code>DATASET</code> can be [<code>CLEARSCOPE_E3</code> | <code>CADETS_E3</code> | <code>THEIA_E3</code>]</p> <p>Note: Large storage capacity is needed to download, parse and save datasets and databases, as well as to run experiments. A single run can generate more than 15GB of artifact files on E3 datasets, and much more with larger E5 datasets.</p>"},{"location":"create-db-from-scratch/#fill-the-database-optc","title":"Fill the database (OpTC)","text":"<p>For OpTC dataset (<code>optc_h051</code>, <code>optc_h201</code> and <code>optc_h501</code>)</p> <p>Still in the container's shell, fill the database for the corresponding dataset by running this command: <pre><code>python dataset_preprocessing/optc/create_database_optc.py orthrus DATASET\n</code></pre> where <code>DATASET</code> can be [<code>optc_h051</code> | <code>optc_h201</code> | <code>optc_h501</code>]</p>"},{"location":"create-db-from-scratch/#verification","title":"Verification","text":"<p>Your databases are now built and filled with data.  If you are not already in the <code>pidsmaker</code> container, run:</p> <pre><code>docker compose -p postgres -f compose-postgres.yml up -d --build\ndocker compose -f compose-pidsmaker.yml up -d --build\ndocker compose exec pids bash\n</code></pre> <p>Then run inside the container:</p> <pre><code>python pidsmaker/main.py orthrus DATASET\n</code></pre> <p>To export your database as a dump file for sharing, do: <pre><code>PGPASSWORD=postgres pg_dump -U postgres -h postgres -p 5432 -F c -d DATASET -f DATASET.dump\n</code></pre></p>"},{"location":"download-files/","title":"Download files","text":""},{"location":"download-files/#downloading-files-manually","title":"Downloading Files Manually","text":"<ol> <li> <p>create a new folder (referred to as the DATA_FOLDER) and download all <code>.gz</code> files from a specific DARPA dataset (follow the link provided for DARPA E3 here and DARPA E5 here). If using CLI, use gdown, by taking the ID of the document directly from the URL. In some cases, the downloading of a file may stop, in this case, simply ctrl+C and re-run the same gdown command with <code>--continue</code> until the file is fully downloaded.  NOTE: Old files should be deleted before  downloading a new dataset.</p> </li> <li> <p>in the DATA_FOLDER, download the java binary (ta3-java-consumer.tar.gz) to build the avro files for DARPA E3 and E5.</p> </li> <li> <p>in the DATA_FOLDER, download the schema files (i.e. files with filename extension '.avdl' and '.avsc') for DARPA E3 and E5.</p> </li> </ol>"},{"location":"introduction/","title":"Introduction","text":"<p>In this introduction, we go through the basics that allow you to run existings PIDSs and create your own variant.</p> <p>Note</p> <p>To follow the tutorial, you should have completed the installation guidelines and open a shell within the pids container.</p>"},{"location":"introduction/#basic-usage-of-the-framework","title":"Basic usage of the framework","text":"<p>The framework currently support the following systems and datasets:</p> <p>Systems</p> <ul> <li><code>velox</code></li> <li><code>orthrus</code></li> <li><code>nodlink</code></li> <li><code>threatrace</code></li> <li><code>kairos</code></li> <li><code>rcaid</code></li> <li><code>flash</code></li> </ul> <p>Datasets</p> <ul> <li><code>CADETS_E3</code></li> <li><code>THEIA_E3</code></li> <li><code>CLEARSCOPE_E3</code></li> <li><code>CADETS_E5</code></li> <li><code>THEIA_E5</code></li> <li><code>CLEARSCOPE_E5</code></li> <li><code>optc_h201</code></li> <li><code>optc_h501</code></li> <li><code>optc_h051</code></li> </ul> <p>Run the framework</p> <p>The basic usage of the framework is:</p> <pre><code>python pidsmaker/main.py SYSTEM DATASET --arg1=x --arg2=y\n</code></pre> <p>The entrypoint is always <code>main.py</code> and only two arguments are mandatory:</p> <ul> <li><code>SYSTEM</code>: should point to an existing YML file with the same system name in <code>config/</code>. The file contains the configuration of the particular system.</li> <li><code>DATASET</code>: should point to an existing dataset defined within <code>DATASET_DEFAULT_CONFIG</code> in <code>config/config.py</code>. It's there that <code>DATASET</code> is mapped to the actual database name, located in the postgres container.</li> </ul> <p>After running the framework, the content of <code>config/SYSTEM.yml</code> is parsed and verified based on the available arguments located in <code>TASK_ARGS</code> from <code>config.py</code>. These arguments are presented in the <code>Arguments</code> section of the documentation.</p> <p>Below are the different ways to run the framework.</p> <ol> <li> <p>Run in the shell, no W&amp;B:     <pre><code>python pidsmaker/main.py SYSTEM DATASET\n</code></pre></p> </li> <li> <p>Run in the shell, monitored to W&amp;B:     <pre><code>python pidsmaker/main.py SYSTEM DATASET --wandb\n</code></pre></p> </li> <li> <p>Run in background, monitored to W&amp;B (ideal for multiple parallel runs):     <pre><code>./run.sh SYSTEM DATASET\n</code></pre>     You can still watch the logs in your shell using <code>tail -f nohup.out</code></p> </li> </ol>"},{"location":"introduction/#device","title":"Device","text":"<p>By default, the framework runs on GPU and searches for an existing device on <code>CUDA:0</code>. If no GPU is detected, it switches to CPU and a warning message is printed to the console. The utilization of CPU can be forced using the <code>--cpu</code> CLI argument.</p>"},{"location":"introduction/#the-framework","title":"The framework","text":"<p>To familiarize yourself with \ud83e\udd77 PIDSMaker, consider going through the pipeline and tutorial pages.</p>"},{"location":"pipeline/","title":"Pipeline","text":"<p>Within PIDSMaker, the execution of each system is broken down into 8 tasks, each of which receives a set of input arguments defined in the system's YAML configuration files located in the <code>config/</code> directory.</p> Main files associated with the pipeline<pre><code>config/                 # existing systems are defined by their own YML file\n\u251c\u2500\u2500 orthrus.yml\n\u251c\u2500\u2500 kairos.yml\n\u2514\u2500\u2500 ...\npidsmaker/\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 config.py       # available arguments to use in YML files\n\u2502   \u2514\u2500\u2500 pipeline.py     # pipeline code\n\u251c\u2500\u2500 preprocessing/               \n\u2502   \u251c\u2500\u2500 build_graphs.py        # task 1. feature extraction + graph TW construction\n\u2502   \u2514\u2500\u2500 transformation.py      # task 2. graph transformation\n\u251c\u2500\u2500 featurization/              \n\u2502   \u251c\u2500\u2500 feat_training.py       # task 3. featurization (word2vec, doc2vec, ...) training\n\u2502   \u2514\u2500\u2500 feat_inference.py      # task 4. featurization inference\n\u251c\u2500\u2500 detection/                  \n\u2502   \u251c\u2500\u2500 graph_preprocessing.py # task 5. batch construction, neighbor sampling, etc\n\u2502   \u251c\u2500\u2500 gnn_training.py        # task 6. GNN training + inference (testing) loop\n\u2502   \u2514\u2500\u2500 evaluation.py          # task 7. metrics calculation + plots\n\u251c\u2500\u2500 triage/       \n\u2502   \u2514\u2500\u2500 tracing.py             # task 8. optional post-processing attack tracing\n</code></pre> <p>Under the hood, PIDSMaker generates a unique hash for each task based on its set of arguments. Once a task completes, its output files are saved to disk in a folder named after this hash. This mechanism allows the system to detect whether a task has already been executed by recomputing the hash from the current arguments and checking for the existence of the corresponding folder.</p> <p>This approach prevents unnecessary recomputation of tasks.</p> <p>Example</p> <p>For instance, if the same YAML configuration is run twice, the first execution will process all tasks (assuming no prior runs with the same arguments), while the second will skip all tasks\u2014since they have already been computed and the results are assumed to be identical. However, if the second run introduces a change\u2014such as modifying the text embedding size <code>emb_dim</code> in the <code>feat_training</code> task\u2014then the pipeline will resume execution starting from <code>feat_training</code>, reusing earlier outputs as appropriate.</p> <p>By reusing previously computed tasks, the pipeline significantly reduces redundant computations, enabling faster and more efficient experimentation.</p>"},{"location":"pipeline/#cli-arguments","title":"CLI arguments","text":"<p>While these YAML files provide the default configuration, they are not the only way to specify task arguments. CLI arguments can also be used. They take precedence over YAML-defined values, meaning that any argument provided via the CLI will override the corresponding value in the YAML file.</p> <pre><code>python pidsmaker/main.py orthrus CADETS_E3 \\\n    --featurization.feat_training.emb_dim=64 \\\n    --detection.gnn_training.lr=0.0001\n</code></pre> <p>The previous command is similar to the following YAML config:</p> <pre><code>featurization:\n    feat_training:\n        emb_dim: 64\ndetection:\n    gnn_training:\n        lr: 0.0001\n</code></pre>"},{"location":"pipeline/#forcing-restart","title":"Forcing restart","text":"<p>During development or experimentation, you may need to restart the pipeline from specific tasks\u2014even when using the same set of arguments. To achieve this, use the <code>--force_restart</code> flag. For example, to restart from the <code>feat_training</code> task, run. <pre><code>python pidsmaker/main.py orthrus CADETS_E3 --force_restart=feat_training\n</code></pre></p> <p>Note</p> <p>Forcing a restart will overwrite any previously generated data associated with the exact same tasks.</p> <p>If you wish to restart the entire pipeline without overwriting previously generated data, you can use:</p> <p><pre><code>python pidsmaker/main.py orthrus CADETS_E3 --restart_from_scratch\n</code></pre> This option instructs the pipeline to generate outputs in a new folder identified by a random hash, ensuring that the run is completely isolated from any prior executions.</p> <p>Warning</p> <p>Since the hash is randomly generated, the outputs of this run cannot be retrieved later based on task arguments. As a result, all files produced when using <code>--restart_from_scratch</code> are deleted at the end of the pipeline to avoid unnecessary disk usage.</p>"},{"location":"release_notes/","title":"Changelog","text":"<p>````</p>"},{"location":"release_notes/#changelog_1","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"release_notes/#unreleased","title":"[Unreleased]","text":""},{"location":"release_notes/#100-2025-06-05","title":"[1.0.0] - 2025-06-05","text":"<ul> <li>Initial release</li> <li>Systems: Velox, Orthrus, R-Caid, Flash, Kairos, Magic, NodLink, ThreaTrace</li> <li>Datasets: CLEARSCOPE_E3, CADETS_E3, THEIA_E3, CLEARSCOPE_E5, THEIA_E5, optc_h201, optc_h501, optc_h051</li> </ul>"},{"location":"singularity_install/","title":"Install Framework using Singularity/Apptainer","text":"<p>For quick installation on environments where Docker is not available (such as HPC clusters), you can use Singularity/Apptainer. This guide assumes Singularity/Apptainer is already installed on your system.</p>"},{"location":"singularity_install/#setup-process","title":"Setup Process","text":""},{"location":"singularity_install/#1-database-setup","title":"1. Database Setup","text":"<p>The Makefile in <code>./scripts/apptainer/Makefile</code> provides easy environment setup:</p> <pre><code>cd ./scripts/apptainer\nmake full-setup\n</code></pre> <p>This command will: - Download and run a PostgreSQL container through Singularity/Apptainer - Load database dumps by executing the <code>load_dumps.sh</code> script</p>"},{"location":"singularity_install/#2-container-management","title":"2. Container Management","text":"<p>Once the database is ready: - Stop the container: <code>make down</code> - Start it again: <code>make up</code></p>"},{"location":"singularity_install/#3-dependencies-installation","title":"3. Dependencies Installation","text":"<p>Install all required dependencies using conda:</p> <pre><code>conda env create -f ./scripts/apptainer/environment.yml\nconda activate pids\n</code></pre>"},{"location":"singularity_install/#running-the-framework","title":"Running the Framework","text":"<p>Once both the database and conda environment are ready, run the framework with:</p> <pre><code>python pidsmaker/main.py SYSTEM DATASET --artifact_dir ./artifacts/ --database_host localhost\n</code></pre>"},{"location":"ten-minute-install/","title":"10-min Docker Install with DARPA TC/OpTC Datasets","text":""},{"location":"ten-minute-install/#download-datasets","title":"Download datasets","text":"<p>DARPA TC and OpTC are very large datasets that are significantly challenging to process. We provide our pre-processed versions of these datasets. We use a postgres database to store and load the data and provide the dumps to download.</p> <p>Sizes for each database dump are as follow: compressed is the raw size of each dump, uncompressed is the size taken once loaded into the postgres table.</p> Dataset Compressed (GB) Uncompressed (GB) <code>CLEARSCOPE_E3</code> 0.6 4.8 <code>CADETS_E3</code> 1.4 10.1 <code>THEIA_E3</code> 1.1 12 <code>CLEARSCOPE_E5</code> 6.2 49 <code>CADETS_E5</code> 36 276 <code>THEIA_E5</code> 5.8 36 <code>OPTC_H051</code> 1.7 7.7 <code>OPTC_H_501</code> 1.5 6.7 <code>OPTC_H201</code> 2 9.1 <p>Steps:</p> <ol> <li> <p>First download the archive(s) into a new <code>data</code> folder. We provide archives containing multiple datasets if their size is small, or provide the dump directly for larger datasets.     On CLI, you can use <code>curl</code> with an authorization token (as explained here):</p> <ul> <li>Go to OAuth 2.0 Playground https://developers.google.com/oauthplayground/</li> <li>In the <code>Select the Scope</code> box, paste <code>https://www.googleapis.com/auth/drive.readonly</code></li> <li>Click <code>Authorize APIs</code> and then <code>Exchange authorization code for tokens</code></li> <li>Copy the Access token</li> <li>Run in terminal</li> </ul> <p>Note: Each call to curl downloads only a part of each file. You should call the same command multiple times to download the archvives at 100%</p> <pre><code>mkdir data &amp;&amp; cd data\n\n# optc_and_cadets_theia_clearscope_e3.tar\ncurl -H \"Authorization: Bearer ACCESS_TOKEN\" -C - https://www.googleapis.com/drive/v3/files/1i7CkK20p21aBp3HGw46o-Uy31JpPC_Yx?alt=media -o optc_and_cadets_theia_clearscope_e3.tar\n\n# theia_clearscope_e5.tar\ncurl -H \"Authorization: Bearer ACCESS_TOKEN\" -C - https://www.googleapis.com/drive/v3/files/1DfolzEa3PVz_6fGZUNEUm0sBP42LB7_1?alt=media -o theia_clearscope_e5.tar\n\n# cadets_e5.dump\ncurl -H \"Authorization: Bearer ACCESS_TOKEN\" -C - https://www.googleapis.com/drive/v3/files/1Xiq7w0Ofz4jZG2PVFuNqi_i0fm28kRcT?alt=media -o cadets_e5.dump\n</code></pre> </li> <li> <p>Then uncompress the archives (this won't increase space)     <pre><code>tar -xvf optc_and_cadets_theia_clearscope_e3.tar\ntar -xvf theia_clearscope_e5.tar\n</code></pre></p> </li> </ol> <p>Alternatively, here are the guidelines to manually create the databases from the official DARPA TC files.</p>"},{"location":"ten-minute-install/#docker-install","title":"Docker Install","text":"<ol> <li> <p>If not installed, install Docker following the steps from the official site and avoid using sudo.</p> </li> <li> <p>Then, install dependencies for CUDA support with Docker:</p> </li> </ol> <pre><code># Add the NVIDIA package repository\ncurl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg\ncurl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\n\n# Update and install\nsudo apt-get update\nsudo apt-get install -y nvidia-container-toolkit\n\n# Restart services\nsudo systemctl restart docker\nsudo nvidia-ctk runtime configure --runtime=docker\nsudo systemctl restart docker\n</code></pre>"},{"location":"ten-minute-install/#load-databases","title":"Load databases","text":"<p>We create two containers: one that runs the postgres database, the other runs the Python env and the pipeline.</p> <ol> <li> <p>Set your paths in .env     <pre><code>cd ..\ncp .env.local .env\n</code></pre></p> <ul> <li> <p>In <code>.env</code>, set <code>INPUT_DIR</code> to the <code>data</code> folder path. Optionally, set <code>ARTIFACTS_DIR</code> to the folder where all generated files will go (multiple GBs).</p> </li> <li> <p>Run the following command and set accordingly <code>HOST_UID</code>, <code>HOST_GID</code> and <code>USER_NAME</code> in <code>.env</code>.     <pre><code>echo \"HOST_UID=$(id -u)\" &amp;&amp; echo \"HOST_GID=$(id -g)\" &amp;&amp; echo \"USER_NAME=$(whoami)\"\n</code></pre></p> </li> <li> <p>Then run:     <pre><code>source .env\n</code></pre></p> </li> <li> <p>Then create the output artifacts folder if it doesn't exist yet and ensure it is owned by your user.     <pre><code>mkdir ${ARTIFACTS_DIR} || chown ${USER_NAME} -R ${ARTIFACTS_DIR}\n</code></pre></p> </li> </ul> </li> <li> <p>Build  and start the database container up:     <pre><code>docker compose -p postgres -f compose-postgres.yml up -d --build\n</code></pre>     Note: each time you modify variables in <code>.env</code>, update env variables using <code>source .env</code> prior to running <code>docker compose</code>.</p> </li> <li> <p>In a terminal, get a shell into this container:     <pre><code>docker compose -p postgres exec postgres bash\n</code></pre></p> </li> <li>If you have enough space to uncompress all datasets locally (135 GB), run this script to load all databases:     <pre><code>./scripts/load_dumps.sh\n</code></pre>     If you have limited space and want to load databases one by one, do:     <pre><code>pg_restore -U postgres -h localhost -p 5432 -d DATASET /data/DATASET.dump\n</code></pre>     !!! note         If you want to parse raw data and create database from scratch, please follow the guideline instead of running the above two commands.</li> <li>Once databases are loaded, we won't need to touch this container anymore:     <pre><code>exit\n</code></pre></li> </ol>"},{"location":"ten-minute-install/#get-into-the-pidsmaker-container","title":"Get into the PIDSMaker container","text":"<p>It is within the <code>pids</code> container that coding and experiments take place.</p> <ol> <li> <p>For VSCode users, we recommend using the dev container extension to directly open VSCode in the container. To do so, simply install the extension, then ctrl+shift+P and Dev Containers: Open Folder in Container.</p> </li> <li> <p>The other alternative is to load the container manually and open a shell directly in your terminal.     <pre><code>docker compose -f compose-pidsmaker.yml up -d --build\ndocker compose exec pids bash\n</code></pre></p> </li> </ol> <p>It's in this container that the python env is installed and where the framework will be used.</p>"},{"location":"ten-minute-install/#weights-biases-interface","title":"Weights &amp; Biases interface","text":"<p>W&amp;B is used as the default interface to visualize and historize experiments, we highly encourage to use it. You can create an account if not already done. Log into your account from CLI by pasting your API key, obtained via your W&amp;B dashboard:</p> <pre><code>wandb login\n</code></pre> <p>Then you can push the logs and results of experiments to the interface using the <code>--wandb</code> arg or when calling <code>./run.sh</code>.</p>"},{"location":"tutorial/","title":"Implement a new PIDS","text":"<p>In this tutorial, we craft a brand new architecture using existing components in the framework and evaluate it on node-level intrusion detection.</p>"},{"location":"tutorial/#architecture","title":"Architecture","text":"<p>Our goal is to implement a new system that satisfies the following requirements:</p> <ul> <li>Compute text embeddings from the textual attributes of entities, such as file paths, process commands, and socket IP addresses.</li> <li>Learn behavior-specific representations for both source and destination nodes.</li> <li>Leverage GraphSAGE layers to capture structural patterns within the provenance graph.</li> <li>Use the node embeddings generated by the encoder as input to a two-layer MLP decoder, training the model in a self-supervised manner to predict edge types\u2014following an approach similar to that of the Kairos and Orthrus systems.</li> <li>In the final step, classify nodes as malicious if their predicted score exceeds a threshold, defined as the maximum loss observed on the validation set.</li> </ul> <p></p> <p>Approach: We will craft a new encoder and intgrate it in the framework in such a way that it can be used from arguments. We will then create a YML config describing our system's pipeline and will execute it.</p>"},{"location":"tutorial/#requirements","title":"Requirements","text":"<ul> <li>a GPU (min 5 GB memory required), or a CPU</li> <li>RAM &gt;20GB</li> <li>storage &gt;10GB</li> <li>follow the installation guidelines and have a shell opened in the pids container</li> </ul>"},{"location":"tutorial/#integrate-a-new-encoder","title":"Integrate a new encoder","text":"<p>In this example, we implement a new encoder that captures whether nodes are source or destination, then uses a GraphSAGE model to capture structural patterns in the provenance graph.</p> encoders/custom_encoder.py<pre><code>import torch.nn as nn\n\nfrom pidsmaker.encoders import SAGE\n\n\nclass CustomEncoder(nn.Module):\n    def __init__(self, in_dim, hid_dim, out_dim, graph_reindexer, activation, dropout, num_layers, device):\n        super().__init__()\n\n        self.src_proj = nn.Linear(in_dim, hid_dim)\n        self.dst_proj = nn.Linear(in_dim, hid_dim)\n\n        self.sage = SAGE(\n            in_dim=hid_dim,\n            hid_dim=hid_dim,\n            out_dim=out_dim,\n            activation=activation,\n            dropout=dropout,\n            num_layers=num_layers,\n        )\n\n        self.graph_reindexer = graph_reindexer\n\n    def forward(self, x_src, x_dst, edge_index, **kwargs):\n        # Project source and destination nodes in a separate embedding space\n        h_src = self.src_proj(x_src)  # (E, d)\n        h_dst = self.dst_proj(x_dst)  # (E, d)\n\n        # Reshape features to (N, d)\n        h_src_N, h_dst_N = self.graph_reindexer.node_features_reshape(edge_index, h_src, h_dst, x_is_tuple=True)\n        h = h_src_N + h_dst_N  # (N, d)\n\n        # Pass them through a SAGE GNN\n        return self.sage(h, edge_index)\n</code></pre> <p>In this example, only two arguments are specific to the encoder and are not shared globally across all encoders: the <code>activation</code> function and the number of GNN layers (<code>num_layers</code>). We want to allow users to specify these parameters via the configuration file to facilitate experimentation with different values. To achieve this, we need to define a new set of arguments tailored specifically for this encoder.</p> config/config.py<pre><code>ENCODERS_CFG = {\n    ...\n    \"custom_encoder\": {\n        \"activation\": str,\n        \"num_layers\": int,\n    },\n}\n</code></pre> <p>Note</p> <p>If an encoder doesn't rely on any custom arguments, simply leave the dict empty: <code>\"custom_encoder\": {}</code>, but every encoder should be defined here, or it will not be recognized by the framework.</p> <p>All logic related to component instantiation is located in <code>factory.py</code>. To integrate our new encoder, we will add a new case in the <code>encoder_factory()</code> function. The existing <code>graph_reindexer</code> instance can be reused. Here, <code>custom_encoder</code> refers to the name of the encoder, which we will later specify in the configuration file.</p> factory.py: encoder_factory()<pre><code>    ...\n    elif method == \"custom_encoder\":\n        encoder = CustomEncoder(\n            in_dim=in_dim,\n            hid_dim=node_hid_dim,\n            out_dim=node_out_dim,\n            dropout=dropout,\n            graph_reindexer=graph_reindexer,\n            activation=activation_fn_factory(\n                cfg.detection.gnn_training.encoder.custom_encoder.activation),\n            num_layers=cfg.detection.gnn_training.encoder.custom_encoder.num_layers,\n            device=device,\n        )\n</code></pre> <p>Our new argument <code>activation</code> can be accessed from the <code>cfg</code> object via <code>cfg.detection.gnn_training.encoder.custom_encoder.activation</code>.</p> <p>Then add the encoder to the list of available encoders in <code>__init__.py</code>.</p> encoders/__init__.py<pre><code>...\nfrom .custom_encoder import CustomEncoder\n</code></pre>"},{"location":"tutorial/#integrate-a-new-system","title":"Integrate a new system","text":"<p>To integrate a new system, first create a new YAML file: <code>config/custom_system.yml</code>. This file describes all the logic of our new PIDS pipeline. In this example, we take <code>orthrus</code> as base configuration. We only override some arguments for simplicity.</p> <p>Question</p> <p>All available arguments for each component and task can be found in the pages under the Configuration section of the documentation.</p> <p>Config:</p> config/custom_system.yml<pre><code>_include_yml: orthrus # (1)!\n\npreprocessing:\n  build_graphs:\n    time_window_size: 15.0 # (2)!\n    node_label_features: # (14)!\n      subject: type, path, cmd_line\n      file: type, path\n      netflow: type, remote_ip, remote_port\n\nfeaturization:\n  feat_training:\n    emb_dim: 128\n    epochs: 50\n    used_method: word2vec # (15)!\n    word2vec:\n      alpha: 0.025\n      window_size: 5\n      min_count: 1\n      use_skip_gram: True\n      num_workers: 1\n      compute_loss: True\n      negative: 5\n      decline_rate: 30\n\ndetection:\n  graph_preprocessing:\n    node_features: node_emb,node_type # (16)!\n    edge_features: none # (17)!\n    intra_graph_batching:\n      used_methods: none # (3)!\n\n  gnn_training:\n    lr: 0.0001 # (4)!\n    node_hid_dim: 128\n    node_out_dim: 128\n\n    encoder:\n      dropout: 0.3\n      used_methods: custom_encoder # (5)!\n      custom_encoder:\n        activation: relu # (6)!\n        num_layers: 3\n\n    decoder:\n      used_methods: predict_edge_type # (7)!\n      predict_edge_type:\n        decoder: edge_mlp # (8)!\n        edge_mlp:\n          src_dst_projection_coef: 2 # (9)!\n          architecture_str: linear(0.5) | relu # (10)!\n\n  evaluation:\n    used_method: node_evaluation # (11)!\n    node_evaluation:\n      threshold_method: max_val_loss # (12)!\n      use_kmeans: False # (13)!\n</code></pre> <ol> <li>In this example, we take the <code>orthrus</code> system as base configuration.</li> <li>We partition the graphs in time windows of size 15 minutes.</li> <li>By default <code>orthrus</code> partitions each time window in even smaller batches of 1024 edges. To discard this behavior, we set <code>used_methods: None</code>.</li> <li>We set some custom hyperparameters for training.</li> <li>It's here that we tell the encoder to use, in this case our new <code>custom_encoder</code>.</li> <li>We set the values for our two defined arguments: <code>activation</code> and <code>num_layers</code>.</li> <li>Our objective is to predict edge types as recent research shows it it the best approach yet.</li> <li>Here, <code>edge_mlp</code> is a MLP designed for edge-level tasks like edge type prediction. It first projects src and dst nodes with different linear layers, then applies the MLP specified in <code>architecture_str</code>.</li> <li>This arg refers to how many times the size of the input should be the output of the src/dst linear layer. Here we want to project to an embedding with double the input size.</li> <li>Our final neural network prior to prediction is a two-layer MLP including relu activation. An additional linear layer is added after the relu to match the output size expected by the objective, here the number of edge types.</li> <li>Refers to node-level detection.</li> <li>We compute the threshold based on the maximum loss seen on the validation set.</li> <li>We use here a simple thresholding, without clustering.</li> <li>We describe here the features assigned to each type of entity.</li> <li>We train a word2vec model and embed each node's features (<code>node_label_features</code>) into a vector of size <code>emb_dim</code>.</li> <li>The features to use as node features during GNN training. Here we concatenate the word2vec embedding and one-hot encoded entity type.</li> <li>Our model doesn't integrate edge features so we do not use any in this example.</li> </ol>"},{"location":"tutorial/#run-the-pipeline","title":"Run the pipeline","text":"<p>In the pids container, you can now run the pipeline. We highly recommend using VSCode with the dev container extension to open your whole workspace in the container to avoid re-building the image after updating the code. Using dev container also enables to run the framework in debug mode using VSCode's debugger.</p> <p>For tiny experiments, you can run the framework locally like so:</p> <pre><code>cd scripts\npython pidsmaker/main.py custom_system CADETS_E3 --project=test_custom_system\n</code></pre> <p>For more practical experimentation, prefer running the framework in background and monitor the logs, figures and metrics in Weights &amp; Biases (W&amp;B). To do so, ensure you have logged to W&amp;B with <code>wandb login</code> and run:</p> <pre><code>./run.sh custom_system CADETS_E3 --project=test_custom_system\n</code></pre> <p>Note</p> <p>Use <code>--cpu</code> to run the framework on CPU instead of GPU.</p>"},{"location":"tutorial/#analyze-results","title":"Analyze results","text":"<p>In the W&amp;B interface, go to the <code>test_custom_system</code> project and you can check real-time logs and metrics being updated to your ongoing run.</p> <p></p> <p>Once finished, we provide some figures to illustrate the ability of the model to differentiate attack and benign nodes.</p> <p></p> <p>Results show a noticeable separation between benign and some malicious nodes on all three attacks in the <code>E3-CADETS</code> dataset. However, the threshold (vertical line) is not adequately located in the space of anomaly scores, leading to 21 TPs and 33 FPs, whereas some nodes can be detected without any FP.</p> <p>System metrics such as GPU memory, RAM, and CPU utilization are automatically captured by W&amp;B and can be visualized on the interface.</p> <p></p>"},{"location":"tutorial/#try-variants","title":"Try variants","text":"<p>While this first version of the model yields relatively satisfactory results, we have no guarantee it has the best performing set of arguments on this dataset. We can easily experiment with multiple variants using CLI args. Depending on your hardware, you can run multiple runs in parallel even on a single GPU (usually up to 3-4 parallel runs with simple architectures on a A100 GPU without major runtime overhead).</p> <pre><code># Remove node type from node features, keep only the word2vec embedding\n./run.sh custom_system CADETS_E3 --project=test_custom_system \\\n    --detection.graph_preprocessing.node_features=node_emb\n\n# Increase node embedding size\n./run.sh custom_system CADETS_E3 --project=test_custom_system \\\n    --detection.gnn_training.node_hid_dim=256 \\\n    --detection.gnn_training.node_out_dim=256\n\n# Reduce the number of GNN layers\n./run.sh custom_system CADETS_E3 --project=test_custom_system \\\n    --detection.gnn_training.encoder.custom_encoder.num_layers=2\n</code></pre> <p>For more advanced hyperparameter exploration, consider using the hyperparameter tuning feature.</p>"},{"location":"config/decoders/","title":"Decoders","text":"<p>Decoders take as input node and/or edge embeddings and pass them though another neural network in such a way that the last layer has a shape that fits the downstream objective. For example, a <code>predict_edge_type</code> objective requires the final shape to be the number of edge types, whereas a <code>reconstruct_node_features</code> objective needs a shape that matches the input features given to the encoder. Decoders are usually much simpler than encoders, and can be customed via <code>edge_mlp</code> for edge-level tasks like <code>predict_edge_type</code> or via <code>node_mlp</code> for node-level tasks like <code>reconstruct_node_features</code>.</p> <ul> <li>edge_mlp <ul> <li>architecture_str: str (1)</li> <li>src_dst_projection_coef: int (2)</li> </ul> </li> <li>node_mlp <ul> <li>architecture_str: str</li> </ul> </li> <li>magic_gat <ul> <li>num_layers: int</li> <li>num_heads: int</li> <li>negative_slope: float</li> <li>alpha_l: float</li> <li>activation: str</li> </ul> </li> <li>nodlink </li> <li>inner_product </li> <li>none </li> </ul> <ol> <li>A string describing a simple neural network. Example: if the encoder's output has shape <code>node_out_dim=128</code>                                 setting <code>architecture_str=linear(2) | relu | linear(0.5)</code> creates this MLP: nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, 128), nn.Linear(128, y).                                 Precisely, in linear(x), x is the multiplier of input neurons. The final layer <code>nn.Linear(128, y)</code> is added automatically such that <code>y</code> is the                                 output size matching the downstream objective (e.g. edge type prediction involves predicting 10 edge types, so the output of the decoder should be 10).</li> <li>Multiplier of input neurons to project src and dst nodes.</li> </ol>"},{"location":"config/encoders/","title":"Encoders","text":"<p>Those are neural network encoders. They can be GNNs (<code>tgn</code>, <code>graph_attention</code>, etc.) in the case where the graph structure is leveraged, but can also be a simple linear layer like the one used in Velox (<code>none</code>) or a more complex custom MLP (<code>custom_mlp</code>). The job of encoders is to compute the node and edge embeddings given the next step to the decoder and objective to compute the loss.</p> <ul> <li>tgn <ul> <li>tgn_memory_dim: int</li> <li>tgn_time_dim: int</li> <li>use_node_feats_in_gnn: bool</li> <li>use_memory: bool</li> <li>use_time_order_encoding: bool</li> <li>project_src_dst: bool</li> </ul> </li> <li>graph_attention <ul> <li>activation: str</li> <li>num_heads: int</li> <li>concat: bool</li> <li>flow: str</li> <li>num_layers: int</li> </ul> </li> <li>sage <ul> <li>activation: str</li> <li>num_layers: int</li> </ul> </li> <li>gat <ul> <li>activation: str</li> <li>num_heads: int</li> <li>concat: bool</li> <li>flow: str</li> <li>num_layers: int</li> </ul> </li> <li>gin <ul> <li>activation: str</li> <li>num_layers: int</li> </ul> </li> <li>sum_aggregation </li> <li>rcaid_gat </li> <li>magic_gat <ul> <li>num_layers: int</li> <li>num_heads: int</li> <li>negative_slope: float</li> <li>alpha_l: float</li> <li>activation: str</li> </ul> </li> <li>glstm </li> <li>custom_mlp <ul> <li>architecture_str: str</li> </ul> </li> <li>none </li> </ul>"},{"location":"config/featurization/","title":"Featurization","text":"<p>Featurization methods are used to transform textual attributes of entities (e.g. file paths, process command lines, socket IP addresses and ports) into a vector. Some methods like <code>word2vec</code> and <code>doc2vec</code> learn this vector from the text corpus, while others like <code>hierarchical_hashing</code> compute the vector in a deterministic way. </p> <p>Other methods like <code>only_type</code> and <code>only_ones</code> simply skip this embedding step and assign either a one-hot encoded type or ones to each entity. Those methods thus do not require any specific argument. In all methods, the resulting vectors are used as node features during the <code>gnn_training</code> task.</p> <ul> <li>word2vec <ul> <li>alpha: float</li> <li>window_size: int</li> <li>min_count: int</li> <li>use_skip_gram: bool</li> <li>num_workers: int</li> <li>epochs: int</li> <li>compute_loss: bool</li> <li>negative: int</li> <li>decline_rate: int</li> </ul> </li> <li>doc2vec <ul> <li>include_neighbors: bool</li> <li>epochs: int</li> <li>alpha: float</li> </ul> </li> <li>fasttext <ul> <li>min_count: int</li> <li>alpha: float</li> <li>window_size: int</li> <li>negative: int</li> <li>num_workers: int</li> <li>use_pretrained_fb_model: bool</li> </ul> </li> <li>alacarte <ul> <li>walk_length: int</li> <li>num_walks: int</li> <li>epochs: int</li> <li>context_window_size: int</li> <li>min_count: int</li> <li>use_skip_gram: bool</li> <li>num_workers: int</li> <li>compute_loss: bool</li> <li>add_paths: bool</li> </ul> </li> <li>temporal_rw <ul> <li>walk_length: int</li> <li>num_walks: int</li> <li>trw_workers: int</li> <li>time_weight: str</li> <li>half_life: int</li> <li>window_size: int</li> <li>min_count: int</li> <li>use_skip_gram: bool</li> <li>wv_workers: int</li> <li>epochs: int</li> <li>compute_loss: bool</li> <li>negative: int</li> <li>decline_rate: int</li> </ul> </li> <li>flash <ul> <li>min_count: int</li> <li>workers: int</li> </ul> </li> <li>hierarchical_hashing </li> <li>magic </li> <li>only_type </li> <li>only_ones </li> </ul>"},{"location":"config/objectives/","title":"Objectives","text":"<p>An objective simply consists in a loss function and a decoder. Node-level objectives compute a loss for every node in a time-window graph, whereas edge-level ones compute loss for all edges. This makes node-level objectives usually faster but less powerful than edge-level objectives to capture pair-wise information.</p>"},{"location":"config/objectives/#arguments","title":"Arguments","text":"<ul> <li>predict_edge_type <ul> <li>decoder: str (1)</li> <li>balanced_loss: bool</li> <li>use_triplet_types: bool</li> </ul> </li> <li>predict_node_type <ul> <li>decoder: str (2)</li> <li>balanced_loss: bool</li> </ul> </li> <li>predict_masked_struct <ul> <li>loss: str (3)</li> <li>decoder: str (4)</li> <li>balanced_loss: bool</li> </ul> </li> <li>detect_edge_few_shot <ul> <li>decoder: str (5)</li> </ul> </li> <li>predict_edge_contrastive <ul> <li>decoder: str (6)</li> </ul> </li> <li>reconstruct_node_features <ul> <li>loss: str (7)</li> <li>decoder: str (8)</li> </ul> </li> <li>reconstruct_node_embeddings <ul> <li>loss: str (9)</li> <li>decoder: str (10)</li> </ul> </li> <li>reconstruct_edge_embeddings <ul> <li>loss: str (11)</li> <li>decoder: str (12)</li> </ul> </li> <li>reconstruct_masked_features <ul> <li>loss: str (13)</li> <li>mask_rate: float</li> <li>decoder: str (14)</li> </ul> </li> </ul> <ol> <li>Decoder used before computing loss.Available options (one selection):<code>edge_mlp</code><code>node_mlp</code><code>magic_gat</code><code>nodlink</code><code>inner_product</code><code>none</code></li> <li>Decoder used before computing loss.Available options (one selection):<code>edge_mlp</code><code>node_mlp</code><code>magic_gat</code><code>nodlink</code><code>inner_product</code><code>none</code></li> <li>Available options (one selection):<code>cross_entropy</code><code>BCE</code></li> <li>Decoder used before computing loss.Available options (one selection):<code>edge_mlp</code><code>node_mlp</code><code>magic_gat</code><code>nodlink</code><code>inner_product</code><code>none</code></li> <li>Decoder used before computing loss.Available options (one selection):<code>edge_mlp</code><code>node_mlp</code><code>magic_gat</code><code>nodlink</code><code>inner_product</code><code>none</code></li> <li>Decoder used before computing loss.Available options (one selection):<code>edge_mlp</code><code>node_mlp</code><code>magic_gat</code><code>nodlink</code><code>inner_product</code><code>none</code></li> <li>Available options (one selection):<code>SCE</code><code>MSE</code><code>MSE_sum</code><code>MAE</code><code>none</code></li> <li>Decoder used before computing loss.Available options (one selection):<code>edge_mlp</code><code>node_mlp</code><code>magic_gat</code><code>nodlink</code><code>inner_product</code><code>none</code></li> <li>Available options (one selection):<code>SCE</code><code>MSE</code><code>MSE_sum</code><code>MAE</code><code>none</code></li> <li>Decoder used before computing loss.Available options (one selection):<code>edge_mlp</code><code>node_mlp</code><code>magic_gat</code><code>nodlink</code><code>inner_product</code><code>none</code></li> <li>Available options (one selection):<code>SCE</code><code>MSE</code><code>MSE_sum</code><code>MAE</code><code>none</code></li> <li>Decoder used before computing loss.Available options (one selection):<code>edge_mlp</code><code>node_mlp</code><code>magic_gat</code><code>nodlink</code><code>inner_product</code><code>none</code></li> <li>Available options (one selection):<code>SCE</code><code>MSE</code><code>MSE_sum</code><code>MAE</code><code>none</code></li> <li>Decoder used before computing loss.Available options (one selection):<code>edge_mlp</code><code>node_mlp</code><code>magic_gat</code><code>nodlink</code><code>inner_product</code><code>none</code></li> </ol>"},{"location":"config/tasks/","title":"Tasks","text":"<p>Tasks are steps composing the pipeline, starting from graph construction (<code>build_graphs</code>) to detection (<code>evaluation</code>) or optionally triage (<code>tracing</code>). Each task takes as input the output from the previous task and write its output to the disk so that the next task can use it. This process enables \"checkpointing\" across the pipeline and avoids the duplication of compute. More information on tasks and the pipeline here.</p>"},{"location":"config/tasks/#preprocessing","title":"Preprocessing","text":"<ul> <li>build_graphs <ul> <li>used_method: str (1)</li> <li>use_all_files: bool</li> <li>mimicry_edge_num: int</li> <li>time_window_size: float (2)</li> <li>use_hashed_label: bool (3)</li> <li>fuse_edge: bool (4)</li> <li>node_label_features <ul> <li>subject: str (5)</li> <li>file: str (6)</li> <li>netflow: str (7)</li> </ul> </li> <li>multi_dataset: str (8)</li> </ul> </li> <li>transformation <ul> <li>used_methods: str (9)</li> <li>rcaid_pseudo_graph <ul> <li>use_pruning: bool</li> </ul> </li> <li>synthetic_attack_naive <ul> <li>num_attacks: int</li> <li>num_malicious_process: int</li> <li>num_unauthorized_file_access: int</li> <li>process_selection_method: str</li> </ul> </li> </ul> </li> </ul> <ol> <li>The method to build time window graphs.Available options (one selection):<code>default</code><code>magic</code></li> <li>The size of each graph in minutes. The notation should always be float (e.g. 10.0). Supports sizes &lt; 1.0.</li> <li>Whether to hash the textual features.</li> <li>Whether to fuse duplicate sequential edges into a single edge.</li> <li>Which features use for process nodes. Features will be concatenated.Available options (multi selection):<code>type</code><code>path</code><code>cmd_line</code></li> <li>Which features use for file nodes. Features will be concatenated.Available options (multi selection):<code>type</code><code>path</code></li> <li>Which features use for netflow nodes. Features will be concatenated.Available options (multi selection):<code>type</code><code>remote_ip</code><code>remote_port</code></li> <li>A comma-separated list of datasets on which training is performed. Evaluation is done only the primary dataset run in CLI.Available options (one selection):<code>THEIA_E5</code><code>THEIA_E3</code><code>CADETS_E5</code><code>CADETS_E3</code><code>CLEARSCOPE_E5</code><code>CLEARSCOPE_E3</code><code>optc_h201</code><code>optc_h501</code><code>optc_h051</code><code>none</code></li> <li>Applies transformations to graphs after their construction. Multiple transformations can be applied sequentially. Example: <code>used_methods=undirected,dag</code>Available options (multi selection):<code>undirected</code><code>dag</code><code>rcaid_pseudo_graph</code><code>none</code><code>synthetic_attack_naive</code></li> </ol>"},{"location":"config/tasks/#featurization","title":"Featurization","text":"<ul> <li>feat_training <ul> <li>emb_dim: int (1)</li> <li>epochs: int (2)</li> <li>use_seed: bool</li> <li>training_split: str (3)</li> <li>multi_dataset_training: bool (4)</li> <li>used_method: str (5)</li> </ul> </li> <li>feat_inference <ul> <li>to_remove: bool</li> </ul> </li> </ul> <ol> <li>Size of the text embedding. Arg not used by some featurization methods that do not build embeddings.</li> <li>Epochs to train the embedding method. Arg not used by some methods.</li> <li>The partition of data used to train the featurization method.Available options (one selection):<code>train</code><code>all</code></li> <li>Whether the featurization method should be trained on all datasets in <code>multi_dataset</code>.</li> <li>Algorithms used to create node and edge features.Available options (one selection):<code>word2vec</code><code>doc2vec</code><code>fasttext</code><code>alacarte</code><code>temporal_rw</code><code>flash</code><code>hierarchical_hashing</code><code>magic</code><code>only_type</code><code>only_ones</code></li> </ol>"},{"location":"config/tasks/#detection","title":"Detection","text":"<ul> <li>graph_preprocessing <ul> <li>save_on_disk: bool (1)</li> <li>node_features: str (2)</li> <li>edge_features: str (3)</li> <li>multi_dataset_training: bool (4)</li> <li>fix_buggy_graph_reindexer: bool (5)</li> <li>global_batching <ul> <li>used_method: str (6)</li> <li>global_batching_batch_size: int (7)</li> <li>global_batching_batch_size_inference: int (8)</li> </ul> </li> <li>intra_graph_batching <ul> <li>used_methods: str (9)</li> <li>edges <ul> <li>intra_graph_batch_size: int (10)</li> </ul> </li> <li>tgn_last_neighbor <ul> <li>tgn_neighbor_size: int (11)</li> <li>tgn_neighbor_n_hop: int (12)</li> <li>fix_buggy_orthrus_TGN: bool (13)</li> <li>fix_tgn_neighbor_loader: bool (14)</li> <li>directed: bool (15)</li> <li>insert_neighbors_before: bool (16)</li> </ul> </li> </ul> </li> <li>inter_graph_batching <ul> <li>used_method: str (17)</li> <li>inter_graph_batch_size: int (18)</li> </ul> </li> </ul> </li> <li>gnn_training <ul> <li>use_seed: bool</li> <li>deterministic: bool (19)</li> <li>num_epochs: int</li> <li>patience: int</li> <li>lr: float</li> <li>weight_decay: float</li> <li>node_hid_dim: int (20)</li> <li>node_out_dim: int (21)</li> <li>grad_accumulation: int (22)</li> <li>inference_device: str (23)</li> <li>used_method: str (24)</li> <li>encoder <ul> <li>dropout: float</li> <li>used_methods: str (25)</li> <li>x_is_tuple: bool (26)</li> </ul> </li> <li>decoder <ul> <li>used_methods: str (27)</li> <li>use_few_shot: bool (28)</li> <li>few_shot <ul> <li>include_attacks_in_ssl_training: bool</li> <li>freeze_encoder: bool</li> <li>num_epochs_few_shot: int</li> <li>patience_few_shot: int</li> <li>lr_few_shot: float</li> <li>weight_decay_few_shot: float</li> <li>decoder <ul> <li>used_methods: str</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>evaluation <ul> <li>viz_malicious_nodes: bool (29)</li> <li>ground_truth_version: str (30)</li> <li>best_model_selection: str (31)</li> <li>used_method: str</li> <li>node_evaluation <ul> <li>threshold_method: str (32)</li> <li>use_dst_node_loss: bool (33)</li> <li>use_kmeans: bool (34)</li> <li>kmeans_top_K: int (35)</li> </ul> </li> <li>tw_evaluation <ul> <li>threshold_method: str (36)</li> </ul> </li> <li>node_tw_evaluation <ul> <li>threshold_method: str (37)</li> <li>use_dst_node_loss: bool</li> <li>use_kmeans: bool</li> <li>kmeans_top_K: int</li> </ul> </li> <li>queue_evaluation <ul> <li>used_method: str (38)</li> <li>queue_threshold: int</li> <li>kairos_idf_queue <ul> <li>include_test_set_in_IDF: bool</li> </ul> </li> <li>provnet_lof_queue <ul> <li>queue_arg: str</li> </ul> </li> </ul> </li> <li>edge_evaluation <ul> <li>malicious_edge_selection: str (39)</li> <li>threshold_method: str (40)</li> </ul> </li> </ul> </li> </ul> <ol> <li>Whether to store the graphs on disk upon building the graphs.                 Used to avoid re-computation of very complex batching operations that take time. Can take up to 300GB storage for CADETS_E5.</li> <li>Node features to use during GNN training. <code>node_type</code> is a one-hot encoded entity type vector,                                      <code>node_emb</code> refers to the embedding generated during the <code>featurization</code> task, <code>only_ones</code> is a vector of ones                                       with length <code>node_type</code>, <code>edges_distribution</code> counts emitted and received edges.Available options (multi selection):<code>node_type</code><code>node_emb</code><code>only_ones</code><code>edges_distribution</code></li> <li>Edge features to used during GNN training. <code>edge_type</code> refers to the system call type, <code>edge_type_triplet</code>                                     considers a same edge type as a new type if source or destination node types are different, <code>msg</code> is the message vector                                     used in the TGN, <code>time_encoding</code> encodes temporal order of events with their timestamps in the TGN, <code>none</code> uses no features.Available options (multi selection):<code>edge_type</code><code>edge_type_triplet</code><code>msg</code><code>time_encoding</code><code>none</code></li> <li>Whether the GNN should be trained on all datasets in <code>multi_dataset</code>.</li> <li>A bug has been found in the first version of the framework, where reindexing graphs in shape (N, d)                                                     slightly modify node features. Setting this to true fixes the bug.</li> <li>Flattens the time window-based graphs into a single large                                 temporal graph and recreate graphs based on the given method. <code>edges</code> creates contiguous graphs of size <code>global_batching_batch_size</code> edges,                                 the same applies for <code>minutes</code>, <code>unique_edge_types</code> builds graphs where each pair of connected nodes share edges with distinct edge types,                                 <code>none</code> uses the default time window-based batching defined in minutes with arg <code>time_window_size</code>.Available options (one selection):<code>edges</code><code>minutes</code><code>unique_edge_types</code><code>none</code></li> <li>Controls the value associated with <code>global_batching.used_method</code> (training+inference).</li> <li>Controls the value associated with <code>global_batching.used_method</code> (inference only).</li> <li>Breaks each previously computed graph into even smaller graphs.                                     <code>edges</code> creates contiguous graphs of size <code>intra_graph_batch_size</code> edges (if a graph has 2000 edges and <code>intra_graph_batch_size=1500</code>                                     creates two graphs: one with 1500 edges, the other with 500 edges), <code>tgn_last_neighbor</code> computes for each graph its associated graph                                     based on the TGN last neighbor loader, namely a new graph where each node is connected with its last <code>tgn_neighbor_size</code> incoming edges.                                    <code>none</code> does not alter any graph.Available options (multi selection):<code>edges</code><code>tgn_last_neighbor</code><code>none</code></li> <li>Controls the value associated with <code>global_batching.used_method</code>.</li> <li>Number of last neighbors to store for each node.</li> <li>If greater than one, will also gather the last neighbors of neighbors.</li> <li>A bug has been in the first version of the framework, where the features of last neighbors not appearing                                                 in the input graph have zero node feature vectors. Setting this arg to true includes the features of all nodes in the TGN graph.</li> <li>We found a minor bug in the original TGN code (https://github.com/pyg-team/pytorch_geometric/issues/10100). This                                                     is an unofficial fix.</li> <li>The original TGN's loader builds graphs in an undirected way. This makes the graphs purely directed.</li> <li>Whether to insert the edges of the current graph before loading last neighbors.</li> <li>Batches multiple graphs into a single large one for parallel training.                                     Does not support TGN. <code>graph_batching</code> batches <code>inter_graph_batch_size</code> together, <code>none</code> doesn't batch graphs.Available options (one selection):<code>graph_batching</code><code>none</code></li> <li>Controls the value associated with <code>inter_graph_batching.used_method</code>.</li> <li>Whether to force PyTorch to use deterministic algorithms.</li> <li>Number of neurons in the middle layers of the encoder.</li> <li>Number of neurons in the last layer of the encoder.</li> <li>Number of epochs to gather gradients before backprop.</li> <li>Device used during testing.Available options (one selection):<code>cpu</code><code>cuda</code></li> <li>Which training pipeline use.Available options (one selection):<code>default</code></li> <li>First part of the neural network. Usually GNN encoders to capture complex patterns.Available options (multi selection):<code>tgn</code><code>graph_attention</code><code>sage</code><code>gat</code><code>gin</code><code>sum_aggregation</code><code>rcaid_gat</code><code>magic_gat</code><code>glstm</code><code>custom_mlp</code><code>none</code></li> <li>Whether to consider nodes differently when being source or destination.</li> <li>Second part of the neural network. Usually MLPs specific to the downstream task (e.g. reconstruction of prediction)Available options (multi selection):<code>predict_edge_type</code><code>predict_node_type</code><code>predict_masked_struct</code><code>detect_edge_few_shot</code><code>predict_edge_contrastive</code><code>reconstruct_node_features</code><code>reconstruct_node_embeddings</code><code>reconstruct_edge_embeddings</code><code>reconstruct_masked_features</code></li> <li>Old feature: need some work to update it.</li> <li>Whether to generate images of malicious nodes' neighborhoods (not stable).</li> <li>Available options (one selection):<code>orthrus</code><code>reapr</code></li> <li>Strategy to select the best model across epochs. <code>best_adp</code> selects the best model based on the highest ADP score, <code>best_discrimination</code>                                         selects the model that does the best separation between top-score TPs and top-score FPs.Available options (one selection):<code>best_adp</code><code>best_discrimination</code></li> <li>Method to calculate the threshold value used to detect anomalies.Available options (one selection):<code>max_val_loss</code><code>mean_val_loss</code><code>threatrace</code><code>magic</code><code>flash</code><code>nodlink</code></li> <li>Whether to consider the loss of destination nodes when computing the node-level scores (maximum loss of a node).</li> <li>Whether to cluster nodes after thresholding as done in Orthrus</li> <li>Number of top-score nodes selected before clustering.</li> <li>Time-window detection. The code is broken and needs work to be updated.Available options (one selection):<code>max_val_loss</code><code>mean_val_loss</code><code>threatrace</code><code>magic</code><code>flash</code><code>nodlink</code></li> <li>Node-level detection where a same node in multiple time windows is                         considered as multiple unique nodes. More realistic evaluation for near real-time detection. The code is broken and needs work to be updated.Available options (one selection):<code>max_val_loss</code><code>mean_val_loss</code><code>threatrace</code><code>magic</code><code>flash</code><code>nodlink</code></li> <li>Queue-level detection as in Kairos. The code is broken and needs work to be updated.Available options (one selection):<code>kairos_idf_queue</code><code>provnet_lof_queue</code></li> <li>The ground truth only contains node-level labels.                     This arg controls the strategy to label edges. <code>src_nodes</code> and <code>dst_nodes</code> consider an edge as malicious if only its source or only its destination                     node is malicious. <code>both</code> labels an edge as malicious if both end nodes are malicious.Available options (one selection):<code>src_node</code><code>dst_node</code><code>both_nodes</code></li> <li>Available options (one selection):<code>max_val_loss</code><code>mean_val_loss</code><code>threatrace</code><code>magic</code><code>flash</code><code>nodlink</code></li> </ol>"},{"location":"config/tasks/#triage","title":"Triage","text":"<ul> <li>tracing <ul> <li>used_method: str (1)</li> <li>depimpact <ul> <li>used_method: str (2)</li> <li>score_method: str (3)</li> <li>workers: int</li> <li>visualize: bool</li> </ul> </li> </ul> </li> </ul> <ol> <li>Post-processing step to reconstruct attack paths or reduce false positives. <code>depimpact</code> is used in Orthrus.Available options (one selection):<code>depimpact</code></li> <li>Available options (one selection):<code>component</code><code>shortest_path</code><code>1-hop</code><code>2-hop</code><code>3-hop</code></li> <li>Available options (one selection):<code>degree</code><code>recon_loss</code><code>degree_recon</code></li> </ol>"},{"location":"features/batching/","title":"Batching & Sampling","text":""},{"location":"features/batching/#batching","title":"Batching","text":"<p>Batching refers to grouping edges, nodes, or graphs into a temporal graph provided as input to the model.</p> <p>We provide three batching strategies that can be configured via dedicated batching arguments.</p> <p>Global Batching: takes as input a large flattened graph comprising all events in the dataset and partitions it into equal-size graphs based on number of edges, minutes, or similar.</p> <p>Intra-graph Batching: applies similar batching as global batching but within each built graph.</p> <p>Inter-graph Batching: groups multiple graphs into a single batch. This batch is a large graph where all graphs are stacked together without any overlap, following the mini-batching strategy from PyG.</p> <p></p> <p>Note</p> <p>All three batching strategies apply sequentially and can be used together.</p>"},{"location":"features/batching/#choosing-the-right-batch-size","title":"Choosing the Right Batch Size","text":"<p>The batch size plays a key role in determining the trade-off between memory usage, speed, and learning effectiveness. Selecting an appropriate batch size requires careful consideration of the graph\u2019s scale, the temporal dynamics, and the available hardware resources.</p> <p>Large Batches</p> <ul> <li>\u2705 Enhance training speed through better GPU parallelization  </li> <li>\u2705 Capture events over longer time periods  </li> <li>\u274c Increase GPU memory usage</li> <li>\u274c May cause high node in-degree, leading to over-squashing (loss of neighbor information)</li> </ul> <p>Small Batches</p> <ul> <li>\u2705 Reduce GPU memory consumption  </li> <li>\u2705 Enable fine-grained neighborhood aggregation  </li> <li>\u274c Extend training time  </li> <li>\u274c Risk missing graph patterns spanning longer time ranges if temporal features are not captured</li> </ul>"},{"location":"features/batching/#tgn-last-neighbor-sampling","title":"TGN Last Neighbor Sampling","text":"<p>In the Temporal Graph Network (TGN) architecture, the objective is to predict edges within a graph batch at time \\(t\\) based on the last neighbors of each node seen in batches happening prior to \\(t\\). Setting <code>tgn_last_neighbor</code> to the argument <code>detection.graph_preprocessing.intra_graph_batching.used_methods</code> enables to pre-compute the TGN graph for each preprocessed graph in the dataset. Specifically, it does not replace the graph directly but adds <code>tgn_*</code> attributes to it, which can be used by the downstream encoder.</p> <p>To use the TGN architecture, you should also add <code>tgn</code> to <code>detection.gnn_training.encoder.used_methods</code> as it enables to properly handle TGN attributes. The <code>TGNEncoder</code> accepts as argument an <code>encoder</code>, defined in the config, which will be applied to the pre-computed TGN graph.</p> <p>Examples of TGN config can be found in <code>kairos.yml</code> and <code>orthrus.yml</code>.</p>"},{"location":"features/batching/#neighbor-sampling","title":"Neighbor Sampling","text":"<p>Info</p> <p>Not implemented yet.</p>"},{"location":"features/instability/","title":"Instability","text":""},{"location":"features/instability/#measure-instability-across-multiple-iterations","title":"Measure instability across multiple iterations","text":"<p>Most systems are prone to instability, with some runs reaching high performance, while others fail dramatically. To quantify this instability, we run the system multiple times and compute the mean and standard deviation of key performance metrics. This can be done easily by using the <code>--experiment=run_n_times</code> tag:</p> <pre><code>./run.sh orthrus CADETS_E3 --tuned --experiment=run_n_times\n</code></pre> <p>This process executes the pipeline N times using the same configuration, with each run starting from a specified task. All parameters can be configured in <code>config/experiments/uncertainty/run_n_times.yml</code>, where the number of iterations is defined by <code>iterations</code>, and the initial task by <code>restart_from</code>.</p> <p>Upon completion of all runs, each metric will be reported in three variants: <code>*_mean</code>, <code>*_std</code>, and <code>*_std_rel</code>, corresponding to the mean, standard deviation, and relative standard deviation (i.e., standard deviation normalized by the mean), respectively.</p>"},{"location":"features/tuning/","title":"Hyperparameter Tuning","text":"<p>PIDSMaker simplifies hyperparameter tuning by combining its efficient pipeline design with the power of W&amp;B Sweeps.</p>"},{"location":"features/tuning/#dataset-specific-tuning","title":"Dataset-specific tuning","text":"<p>Tuning is configured using YAML files, just like system definitions. For example, suppose you've created a new system named <code>my_system</code>, and its configuration is stored in <code>config/my_system.yml</code>. To search for optimal hyperparameters on the <code>THEIA_E3</code> dataset, you can create a new tuning configuration file at <code>config/experiments/tuning/systems/theia_e3/tuning_my_system.yml</code> following the W&amp;B syntax:</p> tuning_my_system.yml<pre><code>method: grid # (1)!\n\nparameters:\n  detection.gnn_training.lr:\n    values: [0.001, 0.0001]\n  detection.gnn_training.node_hid_dim:\n    values: [32, 64, 128, 256]\n  featurization.feat_training.used_method:\n    values: [fasttext, word2vec]\n</code></pre> <ol> <li>Other hyperparameter search strategies like <code>random</code> and <code>bayesian</code> can be used (more info here).</li> </ol> <p>Before starting the hyperparameter search, make sure you are logged in to W&amp;B by running: <code>wandb login</code> inside the container\u2019s shell. Then, launch the hyperparameter tuning with: <code>--tuning_mode=hyperparameters</code>:</p> <pre><code>./run.sh my_system CADETS_E3 --tuning_mode=hyperparameters\n</code></pre> <p>This flag will automatically load the tuning configuration from <code>config/experiments/tuning/systems/theia_e3/tuning_my_system.yml</code>, based on the provided dataset and system names. Any overlapping arguments defined in <code>config/my_system.yml</code> will be overridden by those specified in the tuning file. If the specified tuning file does not exist (i.e., no dataset-specific configuration is available), the pipeline falls back to the default tuning configuration: <code>config/experiments/tuning/systems/default/tuning_default_baselines.yml</code>.</p> <p>Note</p> <p>You can also pass arguments directly via the CLI. CLI arguments always take precedence and will override both the system configuration (<code>config/my_system.yml</code>) and the tuning configuration (<code>tuning_my_system.yml</code>).</p>"},{"location":"features/tuning/#cross-dataset-tuning","title":"Cross-dataset tuning","text":"<p>If you wish to use the same hyperparameter tuning configuration across all datasets, you can explicitly specify the tuning file as a command-line argument. For instance, you might create a tuning file named <code>config/experiments/tuning/systems/default/tuning_my_system_all_datasets.yml</code>, then apply it to all dataset by running:</p> <pre><code>./run_all_datasets.py my_system \\\n    --tuning_mode=hyperparameters \\\n    --tuning_file_path=systems/default/tuning_my_system_all_datasets\n</code></pre> <p>or on a single dataset:</p> <pre><code>./run.sh my_system CLEARSCOPE_E3 \\\n    --tuning_mode=hyperparameters \\\n    --tuning_file_path=systems/default/tuning_my_system_all_datasets\n</code></pre> <p>The path passed to <code>--tuning_file_path</code> should start from <code>systems/</code>.</p> <p>Tips</p> <p>For a better historization of experiments, we recommend to assign a name to each sweep (<code>--exp</code>) and a dedicated project name (<code>--project</code>):</p> <pre><code>./run.sh my_system CADETS_E3 \\\n    --tuning_mode=hyperparameters \\\n    --exp=bench_fasttext_word2vec \\\n    --project=best_featurization_method\n</code></pre>"},{"location":"features/tuning/#best-model-selection","title":"Best model selection","text":"<p>Once the sweep has finished, the best run can be obtained from W&amp;B by sorting based on your desired metric.</p> <p></p> <p>Get the hyperparameters associated with the best run and put them into a <code>config/tuned_baselines/{dataset}/tuned_my_system.yml</code>.</p> tuned_my_system.yml<pre><code>featurization:\n  feat_training:\n    used_method: word2vec\n\ndetection:\n  gnn_training:\n    lr: 0.0001\n    node_hid_dim: 128\n</code></pre> <p>Each system should have a tuned file per dataset, or the best hyperparameters can be directly set in its <code>my_system.yml</code> file if it uses the same hyperparameters in all datasets.</p> <p>Note</p> <p>The process of best hyperparameter selection is currently done by hand but will likely be automated in future.</p>"},{"location":"features/tuning/#run-a-tuned-system","title":"Run a tuned system","text":"<p>Running a system with its best hyperparameters on a particular dataset is as using the <code>--tuned</code> arg:</p> <pre><code>./run.sh my_system CADETS_E3 --tuned\n</code></pre> <p>This will search for the <code>config/tuned_baselines/cadets_e3/tuned_my_system.yml</code> file and override the default system config by the best hyperparameters.</p>"}]}